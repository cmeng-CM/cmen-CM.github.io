---
abbrlink: '0'
---
当你与 AI 团队合作时， 你可能会听到他们指的是他们用来构建这些 AI 系统的工具。 在本视频中，我想与您分享一些 最常用的 AI 工具的细节和名称 ，以便您能够更好地了解这些 AI 工程师在做什么。 我们很幸运，当今的人工智能世界非常开放， 许多团队将公开地彼此分享想法。
从 ::30 开始播放视频并学习脚本0:30
许多团队将使用很棒的机器学习、开源工具和框架来构建系统。
从 ::37 开始播放视频并学习脚本0:37
因此，如果你听过这些 PyTorch、TensorFlow、 Hugging Face PaddlePaddle、Scikit-Learn 或 R 中的任何一个， 所有这些都是开源机器学习工具或框架，可以帮助人工智能团队提高效率。 许多AI技术突破也 在这个名为archive的网站上免费发布在Internet上，拼写是这样的。
从 :1: 开始播放视频并学习脚本1:00
我希望其他学术界也能自由分享他们的研究， 因为我亲眼目睹了这在多大程度上加速了整个 AI 领域的进展。 最后，许多团队还将在互联网上自由共享他们的代码， 最常见的是在一个名为GitHub的网站上。 这已成为 AI 和 AI 其他领域开源软件的事实上的存储库。 而且，通过使用获得适当许可的开源软件，许多团队 可以比从头开始构建所有东西的速度快得多。 因此，举例来说，如果我在 GitHub上在线搜索人脸识别软件， 你可能会找到这样的网页。 如果你向下滚动，它实际上对 这个网站上提供的软件有一个相当好的、非常合理的描述。 用于识别人们的面孔，甚至可以找到人脸的一部分。 只有大量的软件可以下载，用于在 互联网上做各种事情，只需仔细检查许可证即可。 当然，在产品中使用许可之前，AI团队会仔细检查许可证。 但是很多软件都是开源的， 或者是经过非常许可的，任何人都可以使用。 尽管GitHub是一个专为工程师打造的技术网站，但如果你愿意， 你应该随意在GitHub上玩耍， 看看人们还在线发布了哪些其他类型的人工智能软件。 除了这些开源技术工具外， 你还经常听到 AI 工程师谈论 CPU 和 GPU。 以下是这些术语的含义。 CPU 是计算机中的计算机处理器，无论是台式 机、笔记本电脑还是云端的计算服务器。 CPU 代表中央处理单元，CPU 由英特尔 和 AMD 以及其他几家公司制造，它会在您的计算机中进行大量计算。 GPU 代表图形处理单元。 从历史上看，GPU 是为处理图片而设计的，所以如果你玩电子游戏， 可能是 GPU 在绘制精美的图形。 但是几年前我们发现，最初 为处理图形而构建的硬件事实证明非常强大。 用于构建超大型神经网络或超大型深度学习算法。 鉴于需要构建超大型深度学习或 超大型神经网络系统。 AI 社区一直渴望越来越多的 计算能力来训练越来越大的神经网络。 事实证明，GPU 非常适合我们训练超大型神经网络所需的这种计算。 因此，这就是 GPU 在深度学习兴起中发挥重要作用的原因。 Nvidia是一家一直在销售许多GPU的公司，但 包括高通在内的其他公司以及谷歌都在生产自己的TPU。 越来越多地制造专门的硬件来 为这些超大型神经网络提供动力。 最后，您可能会听到有关云部署与本地部署的对比，或者 简而言之，本地部署。 云部署是指你租用计算服务器， 例如从亚马逊的AWS或微软的欧元或谷歌的GCP租用计算服务器， 以便使用他人的服务来进行计算。 而本地部署意味着购买自己的计算服务并在 自己的公司本地运行该服务。 详细探讨 这两个选项的优缺点超出了本视频的范围。 世界上许多地方都在转向云部署，但是如果你在网上搜索， 你会发现很多文章都在谈论 云部署与本地部署的优缺点。 你可能会听到最后一个术语，那就是边缘部署。 如果你正在建造一辆自动驾驶汽车，那么没有足够的时间将数据从 自动驾驶汽车发送到云服务器来决定是否应该停车。 然后将该信息发送回自动驾驶汽车。 因此，计算通常必须在车内的计算机上进行， 这就是所谓的边缘部署。 您可以将处理器放在收集数据的地方 ，这样您就可以处理数据并快速做出决定，而无需通过 互联网将数据传输到其他地方进行处理。 如果你也看看家里的一些智能扬声器， 这也是一种边缘部署，有些，不是全部。 但是有些语音识别任务是由处理器完成的，该处理器内置 在家中的智能扬声器中。 边缘部署的主要优势是它可以增加 系统的响应时间，还可以减少您需要通过网络发送的数据量。 但是，边缘部署与云部署与 本地部署也有许多优缺点，您也可以在线搜索以了解更多信息。 感谢您 完成这段关于 AI 工程师使用的技术工具的可选视频。 希望当你听到他们提到其中一些工具时， 你会开始更好地理解它们的意思。 我期待下周见到你。